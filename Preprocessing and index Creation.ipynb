{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a717692a-bc9a-4704-8b41-b4b29473c2f1",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca29c597-564c-48e6-9371-d75604e4e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5266b9ad-b3c4-433c-98da-46cf29b04169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Muneeb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Muneeb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b7ff0a-97c7-4448-8bd8-d815d96e53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert text to lowercase\n",
    "def convert_to_lowercase(directory):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        content = content.lower()\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"All documents have been converted to lowercase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c6a7cc-2448-442d-a544-e008a0bd95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load stopwords from a file\n",
    "def load_stopwords(stopword_file):\n",
    "    stopwords = []\n",
    "    with open(stopword_file, 'r') as file:\n",
    "        for line in file:\n",
    "            cleaned_word = line.strip()\n",
    "            if cleaned_word:\n",
    "                stopwords.append(cleaned_word)\n",
    "    print(\"Stopwords array created!\")\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4fa19c7-1ec2-453f-b662-a12630524bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords from documents\n",
    "def remove_stopwords(directory, stopwords):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        for word in stopwords:\n",
    "            content = content.replace(f' {word} ', ' ')\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"Stopwords removed from all documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b7139df-d145-4420-855a-258872c52031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuations from documents\n",
    "def remove_punctuations(directory):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        content = re.sub(r'[^A-Za-z]+', ' ', content)\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"Punctuations removed from all documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6164553e-b891-45b8-9e18-3da81c04ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Lemmatization to documents \n",
    "def apply_lemmatization(directory):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        tokens = word_tokenize(content)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if len(word) > 1]\n",
    "        lemmatized_content = ' '.join(lemmatized_tokens)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(lemmatized_content)\n",
    "    print(\"Lemmatization applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c5afc40-8a44-43a0-94df-a0c7aa595039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to execute all preprocessing steps\n",
    "def preprocess_documents(directory, stopword_file):\n",
    "    convert_to_lowercase(directory)\n",
    "    stopwords = load_stopwords(stopword_file)\n",
    "    remove_stopwords(directory, stopwords)\n",
    "    remove_punctuations(directory)\n",
    "    apply_lemmatization(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25facef0-42c8-4a32-a299-abf1ebbffcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been converted to lowercase.\n",
      "Stopwords array created!\n",
      "Stopwords removed from all documents!\n",
      "Punctuations removed from all documents!\n",
      "Lemmatization applied successfully!\n"
     ]
    }
   ],
   "source": [
    "abstract='Abstracts'\n",
    "stopword='Stopword-List.txt'\n",
    "preprocess_documents(abstract, stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3d0b7-3b93-4c50-8696-3beb3266130f",
   "metadata": {},
   "source": [
    "# Creating Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06d78314-fe2b-465b-97c2-30c5955e38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cea5e228-e7f6-4e04-9c18-a03bb65999f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bdf9165-1805-4c94-a7e9-c6d2a97315a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir):\n",
    "    files = os.listdir(dir)\n",
    "    pairs = [(int(f.replace('.txt', '')), f) for f in files]\n",
    "    pairs.sort()\n",
    "    return [p[1] for p in pairs]\n",
    "\n",
    "def save_index(index, file):\n",
    "    with open(file, 'w') as f:\n",
    "        for term, docs in index.items():\n",
    "            f.write(f\"{term} : {docs}\\n\")\n",
    "    print(f\"Index saved to {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b730a76f-0963-46c1-82b7-c79fbc326dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Inverted Index\n",
    "def build_inv_index(dir, files):\n",
    "    inv_index = {}\n",
    "    for f in files:\n",
    "        with open(os.path.join(dir, f), 'r') as file:\n",
    "            text = file.read()\n",
    "        words = word_tokenize(text)\n",
    "        doc_id = int(f.replace('.txt', ''))\n",
    "        for word in words:\n",
    "            if word not in inv_index:\n",
    "                inv_index[word] = [doc_id]\n",
    "            elif doc_id not in inv_index[word]:\n",
    "                inv_index[word].append(doc_id)\n",
    "    return dict(sorted(inv_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4999bf6-6332-46f0-b5e8-eef9b6891ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Term-to-Index Mapping for Vector Space\n",
    "def build_term_index_map(dir, files):\n",
    "    term_index = {}\n",
    "    vocab = set()\n",
    "    \n",
    "    # First pass to build vocabulary\n",
    "    for f in files:\n",
    "        with open(os.path.join(dir, f), 'r') as file:\n",
    "            text = file.read()\n",
    "        words = word_tokenize(text)\n",
    "        vocab.update(words)\n",
    "    \n",
    "    # Create term to index mapping\n",
    "    for idx, term in enumerate(sorted(vocab)):\n",
    "        term_index[term] = idx\n",
    "    \n",
    "    return term_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91c7aa22-528e-47f8-b35a-6d3e8c9b69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Vector Space Index (tf-idf vectors)\n",
    "def build_vector_space_index(dir, files, term_index):\n",
    "    num_docs = len(files)\n",
    "    vocab_size = len(term_index)\n",
    "    doc_vectors = []\n",
    "    df = {}  # document frequency\n",
    "    \n",
    "    # Initialize document frequency counts\n",
    "    for term in term_index:\n",
    "        df[term] = 0\n",
    "    \n",
    "    # First pass to calculate document frequencies\n",
    "    for f in files:\n",
    "        with open(os.path.join(dir, f), 'r') as file:\n",
    "            text = file.read()\n",
    "        words = word_tokenize(text)\n",
    "        unique_words = set(words)\n",
    "        for term in unique_words:\n",
    "            if term in term_index:\n",
    "                df[term] += 1\n",
    "    \n",
    "    # Second pass to create vectors\n",
    "    for f in files:\n",
    "        doc_id = int(f.replace('.txt', ''))\n",
    "        with open(os.path.join(dir, f), 'r') as file:\n",
    "            text = file.read()\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # Initialize document vector\n",
    "        vector = np.zeros(vocab_size)\n",
    "        tf = {}  # term frequency in this doc\n",
    "        \n",
    "        # Calculate term frequencies\n",
    "        for word in words:\n",
    "            if word in term_index:\n",
    "                if word in tf:\n",
    "                    tf[word] += 1\n",
    "                else:\n",
    "                    tf[word] = 1\n",
    "        \n",
    "        # Calculate tf-idf weights\n",
    "        for term, freq in tf.items():\n",
    "            term_idx = term_index[term]\n",
    "            idf = np.log(num_docs / (1 + df[term]))  # smoothed idf\n",
    "            vector[term_idx] = freq * idf\n",
    "        \n",
    "        doc_vectors.append(vector)\n",
    "    \n",
    "    return np.array(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43626a77-d02f-4144-9076-9f370a516ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = 'Abstracts'\n",
    "files = get_files(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57a1e039-0292-4c13-98ce-17c2411ef373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms in inverted index: 5860\n",
      "Index saved to inverted_index.txt\n"
     ]
    }
   ],
   "source": [
    "# 1. Building inverted index\n",
    "inv_index = build_inv_index(abstract, files)\n",
    "print(\"Terms in inverted index:\", len(inv_index))\n",
    "save_index(inv_index, \"inverted_index.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54007302-e561-4c88-801a-4e1d89bc6166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5860\n",
      "Index saved to term_index_map.txt\n"
     ]
    }
   ],
   "source": [
    "# 2. Build term-to-index mapping\n",
    "term_index = build_term_index_map(abstract, files)\n",
    "print(\"Vocabulary size:\", len(term_index))\n",
    "save_index(term_index, \"term_index_map.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7525d5c1-96ac-4190-903d-b789f2c24c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector space shape: (448, 5860)\n"
     ]
    }
   ],
   "source": [
    " # 3. Build vector space index\n",
    "vector_space = build_vector_space_index(abstract, files, term_index)\n",
    "print(\"Vector space shape:\", vector_space.shape)\n",
    "np.save(\"vector_space_index.npy\", vector_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6d0aecb-c038-4b9f-a85f-34dd5c985699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example search in vector space\n",
    "def vector_space_search(term, term_index, vector_space, files):\n",
    "    if term in term_index:\n",
    "        term_idx = term_index[term]\n",
    "        print(f\"\\nTerm '{term}' found at index {term_idx}\")\n",
    "        print(\"Document vectors (tf-idf weights) for this term:\")\n",
    "        for doc_idx in range(len(files)):\n",
    "            weight = vector_space[doc_idx][term_idx]\n",
    "            if weight > 0:\n",
    "                print(f\"Doc {files[doc_idx]}: {weight:.4f}\")\n",
    "    else:\n",
    "        print(f\"Term '{term}' not found in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38dd7805-719b-45d6-acae-1c80b42a2b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Term 'bootstrap' found at index 594\n",
      "Document vectors (tf-idf weights) for this term:\n",
      "Doc 181.txt: 14.1555\n",
      "Doc 193.txt: 28.3110\n",
      "Doc 379.txt: 4.7185\n"
     ]
    }
   ],
   "source": [
    "vector_space_search(\"bootstrap\", term_index, vector_space, files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
