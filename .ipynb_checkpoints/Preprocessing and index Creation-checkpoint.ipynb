{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a717692a-bc9a-4704-8b41-b4b29473c2f1",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29c597-564c-48e6-9371-d75604e4e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b7ff0a-97c7-4448-8bd8-d815d96e53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert text to lowercase\n",
    "def convert_to_lowercase(directory):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        content = content.lower()\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"All documents have been converted to lowercase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c6a7cc-2448-442d-a544-e008a0bd95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load stopwords from a file\n",
    "def load_stopwords(stopword_file):\n",
    "    stopwords = []\n",
    "    with open(stopword_file, 'r') as file:\n",
    "        for line in file:\n",
    "            cleaned_word = line.strip()\n",
    "            if cleaned_word:\n",
    "                stopwords.append(cleaned_word)\n",
    "    print(\"Stopwords array created!\")\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4fa19c7-1ec2-453f-b662-a12630524bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords from documents\n",
    "def remove_stopwords(directory, stopwords):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        for word in stopwords:\n",
    "            content = content.replace(f' {word} ', ' ')\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"Stopwords removed from all documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b16ab5-8aae-44e6-b6d9-724041d5eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle edge cases using regex\n",
    "def handle_edge_cases(directory, stopwords):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        for word in stopwords:\n",
    "            pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "            content = re.sub(pattern, ' ', content)\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"Edge cases handled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b7139df-d145-4420-855a-258872c52031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuations from documents\n",
    "def remove_punctuations(directory):\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        content = re.sub(r'[^A-Za-z]+', ' ', content)\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "    print(\"Punctuations removed from all documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6164553e-b891-45b8-9e18-3da81c04ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Porter Stemmer to documents\n",
    "def apply_stemming(directory):\n",
    "    stemmer = PorterStemmer()\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        tokens = word_tokenize(content)\n",
    "        stemmed_tokens = [stemmer.stem(word) for word in tokens if len(word) > 1]\n",
    "        stemmed_content = ' '.join(stemmed_tokens)\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(stemmed_content)\n",
    "    print(\"Porter Stemmer applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c5afc40-8a44-43a0-94df-a0c7aa595039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to execute all preprocessing steps\n",
    "def preprocess_documents(directory, stopword_file):\n",
    "    convert_to_lowercase(directory)\n",
    "    stopwords = load_stopwords(stopword_file)\n",
    "    remove_stopwords(directory, stopwords)\n",
    "    handle_edge_cases(directory, stopwords)\n",
    "    remove_punctuations(directory)\n",
    "    apply_stemming(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25facef0-42c8-4a32-a299-abf1ebbffcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been converted to lowercase.\n",
      "Stopwords array created!\n",
      "Stopwords removed from all documents!\n",
      "Edge cases handled!\n",
      "Punctuations removed from all documents!\n",
      "Porter Stemmer applied successfully!\n"
     ]
    }
   ],
   "source": [
    "abstract='Abstracts'\n",
    "stopword='Stopword-List.txt'\n",
    "preprocess_documents(abstract, stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3d0b7-3b93-4c50-8696-3beb3266130f",
   "metadata": {},
   "source": [
    "# Creating Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b730a76f-0963-46c1-82b7-c79fbc326dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4999bf6-6332-46f0-b5e8-eef9b6891ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Muneeb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "abstracts = 'Abstracts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cd2f734-df77-4f2f-b1b7-1bbe7642e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted([file for file in os.listdir(abstracts) if file.endswith('.txt')],\n",
    "               key=lambda x: int(x.replace('.txt', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57a1e039-0292-4c13-98ce-17c2411ef373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build inverted index\n",
    "inverted_index = defaultdict(list)\n",
    "for file in files:\n",
    "    with open(os.path.join(abstracts, file), 'r') as f:\n",
    "        doc_id = int(file.replace('.txt', ''))\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if doc_id not in inverted_index[word]:\n",
    "                inverted_index[word].append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38dd7805-719b-45d6-acae-1c80b42a2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save sorted inverted index to file\n",
    "with open('inverted_index.txt', 'w') as f:\n",
    "    for word, doc_ids in sorted(inverted_index.items()):\n",
    "        f.write(f\"{word}: {', '.join(map(str, doc_ids))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd8a6883-9d3c-4b77-aded-4339053b4e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terms in index: 4211\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terms in index:\", len(inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "473c81e5-9046-4d95-871e-38892cc5203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Query: autoencod\n",
      "Query result: [187, 273, 279, 325, 333, 405]\n"
     ]
    }
   ],
   "source": [
    "#Query processing with Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "query = \"autoencoders\"\n",
    "stemmed_query = porter_stemmer.stem(query)\n",
    "print(f\"Stemmed Query: {stemmed_query}\")\n",
    "\n",
    "if stemmed_query in inverted_index:\n",
    "    print(\"Query result:\", inverted_index[stemmed_query])\n",
    "else:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce610fe7-3818-4a92-9a20-7c44519724af",
   "metadata": {},
   "source": [
    "# Creating Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d357a7f-1c82-4601-9930-49ae03e64c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort files by their numeric prefix\n",
    "files = sorted([file for file in os.listdir(abstracts) if file.endswith('.txt')],\n",
    "               key=lambda x: int(x.replace('.txt', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ef23c03-de49-4dc3-aa61-57b60b906646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the positional index\n",
    "positional_index = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for file in files:\n",
    "    with open(os.path.join(abstracts, file), 'r') as f:\n",
    "        doc_id = int(file.replace('.txt', ''))\n",
    "        terms = word_tokenize(f.read())\n",
    "        position = 0  # Track positions of terms\n",
    "        for term in terms:\n",
    "            positional_index[term][doc_id].append(position)\n",
    "            position += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b05f76d-8962-4b48-a877-29670ff25973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Index Saved\n"
     ]
    }
   ],
   "source": [
    "#save the positional index to a file\n",
    "with open('positional_Index.txt', 'w') as f:\n",
    "    for term, doc_ids in sorted(positional_index.items()):\n",
    "        f.write(f\"{term}: {dict(doc_ids)}\\n\")\n",
    "print(\"Positional Index Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "315848f2-bb74-4ec0-a1c8-601bb6b46fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terms in index: 4211\n"
     ]
    }
   ],
   "source": [
    "print(\"Total terms in index:\", len(positional_index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
